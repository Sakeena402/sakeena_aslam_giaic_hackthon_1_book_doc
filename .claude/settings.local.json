{
  "permissions": {
    "allow": [
      "Bash(pwsh -Command \".specify/scripts/powershell/create-new-feature.ps1 -Json -Number 1 -ShortName ''ros2-nervous-system'' -Description ''/sp.specify\\\\n\\\\nModule: Module 01 — The Robotic Nervous System \\(ROS 2\\)\\\\n\\\\nCourse: Physical AI & Humanoid Robotics\\\\nTheme: AI Systems in the Physical World \\(Embodied Intelligence\\)\\\\n\\\\nModule Purpose:\\\\nThis module introduces ROS 2 as the core nervous system of humanoid robots.\\\\nLearners will understand how software intelligence communicates with physical\\\\nrobot components through nodes, topics, services, and robot descriptions.\\\\nThe module bridges AI agents written in Python with real and simulated robot\\\\ncontrollers.\\\\n\\\\nTarget Audience:\\\\n- Computer science and AI students\\\\n- Robotics beginners with Python knowledge\\\\n- Learners transitioning from pure AI/software to Physical AI systems\\\\n\\\\nLearning Outcomes:\\\\nAfter completing this module, the reader will be able to:\\\\n- Explain ROS 2 architecture and middleware concepts\\\\n- Create and reason about ROS 2 nodes, topics, and services\\\\n- Connect Python-based AI agents to ROS 2 controllers using rclpy\\\\n- Understand and read humanoid robot descriptions written in URDF\\\\n- Conceptually map AI decision-making to physical robot motion\\\\n\\\\nModule Structure \\(Docusaurus\\):\\\\nThis module must be implemented as a Docusaurus section containing\\\\nexactly three chapters:\\\\n\\\\nChapter 1: ROS 2 as the Robotic Nervous System\\\\n- Conceptual overview of ROS 2\\\\n- Why ROS 2 is required for Physical AI\\\\n- Nodes, topics, services, and message passing\\\\n- Analogy between human nervous system and ROS 2 architecture\\\\n- Minimal diagrams \\(SVG or Mermaid\\) for clarity\\\\n\\\\nChapter 2: Python Agents and ROS 2 Communication\\\\n- Role of Python in robotics and AI integration\\\\n- Using rclpy to create ROS 2 nodes\\\\n- Publishing and subscribing to topics\\\\n- Calling and exposing services\\\\n- Bridging AI logic \\(decision-making\\) to motor and sensor control\\\\n- Clear, beginner-friendly code snippets \\(Python\\)\\\\n\\\\nChapter 3: Humanoid Robot Description with URDF\\\\n- Purpose of URDF in humanoid robotics\\\\n- Links, joints, and coordinate frames\\\\n- How URDF connects software to physical structure\\\\n- High-level explanation of humanoid kinematics\\\\n- Preparing robot models for simulation \\(Gazebo / Isaac readiness\\)\\\\n\\\\nContent Standards:\\\\n- Explanations must prioritize clarity over mathematical depth\\\\n- Concepts should be explained with physical-world analogies\\\\n- All technical terms must be defined on first use\\\\n- Code examples must be minimal, readable, and commented\\\\n- No unexplained jumps in difficulty\\\\n\\\\nUI / UX Requirements \\(Docusaurus\\):\\\\n- Minimal and modern visual theme\\\\n- Clean typography with strong heading hierarchy\\\\n- Wide margins and readable line length\\\\n- Inline code blocks with clear syntax highlighting\\\\n- Collapsible sections for advanced notes\\\\n- Diagrams embedded close to explanations\\\\n- Responsive layout for desktop and tablet\\\\n- Optional light/dark mode compatibility\\\\n\\\\nSuccess Criteria:\\\\n- Reader understands ROS 2 without prior robotics experience\\\\n- Reader can mentally trace data flow from AI logic to robot motion\\\\n- Chapters progress logically from concepts → communication → embodiment\\\\n- Module feels visually clean, structured, and easy to navigate\\\\n- Content is consistent with later modul''\")",
      "Bash(pwsh -Command \".specify/scripts/powershell/setup-plan.ps1 -Json\")",
      "Bash(mkdir:*)",
      "Bash(git fetch:*)",
      "Bash(npx docusaurus start --port 3001)",
      "Bash(find specs -name \"tasks.md\" -exec grep -l \"\\\\- \\\\[ \\\\]\" {} ;)",
      "Bash(.specify/scripts/powershell/create-new-feature.ps1 -Json -Number 4 -ShortName \"isaac-ai-brain\" -Description \"/sp.specify\n\nModule: Module 03 — The AI-Robot Brain \\(NVIDIA Isaac™\\)\n\nCourse: Physical AI & Humanoid Robotics\nTheme: Embodied Intelligence and AI-Driven Perception\n\nModule Purpose:\nThis module introduces NVIDIA Isaac as the AI brain of humanoid robots.\nLearners will understand how advanced perception, navigation, and training\npipelines enable robots to see, localize, and move intelligently in complex\nenvironments using hardware-accelerated robotics frameworks.\n\nTarget Audience:\n- AI and robotics students with ROS 2 and simulation background\n- Learners interested in perception-driven robotics\n- Developers transitioning from simulation to AI-powered autonomy\n\nLearning Outcomes:\nAfter completing this module, the reader will be able to:\n- Explain the role of NVIDIA Isaac in Physical AI systems\n- Understand photorealistic simulation and synthetic data generation\n- Conceptually use Isaac ROS for perception and VSLAM\n- Understand Nav2 for humanoid navigation and path planning\n- Describe how AI perception connects to motion and autonomy\n\nModule Structure \\(Docusaurus\\):\nThis module must be implemented as a Docusaurus section containing\nexactly three chapters:\n\nChapter 1: NVIDIA Isaac and the AI-Robot Brain\n- What NVIDIA Isaac is and why it matters\n- Role of accelerated computing in robotics\n- Isaac Sim vs Isaac ROS \\(high-level comparison\\)\n- From simulation to real-world deployment\n- Position of Isaac in the Physical AI stack\n\nChapter 2: Perception and Localization with Isaac ROS\n- Visual perception in humanoid robots\n- Concept of Visual SLAM \\(VSLAM\\)\n- Sensor fusion at a high level\n- Hardware-accelerated perception pipelines\n- Mapping and localization in dynamic environments\n\nChapter 3: Navigation and Intelligent Movement \\(Nav2\\)\n- What Nav2 is and why it is needed\n- Path planning vs obstacle avoidance\n- Navigation for bipedal and humanoid robots \\(conceptual\\)\n- Integration of perception, maps, and motion\n- Preparing for autonomous behavior\n\nContent Standards:\n- Focus on system-level understanding, not low-level tuning\n- All technical terms explained on first use\n- Visual diagrams encouraged for pipelines and data flow\n- Avoid unnecessary mathematical depth\n- Concepts must connect clearly to previous modules\n\nUI / UX Requirements \\(Docusaurus\\):\n- Minimal and modern aesthetic\n- Clear heading hierarchy and readable fonts\n- Diagrams placed near explanations\n- Collapsible sections for advanced concepts\n- Clean, distraction-free layout\n- Responsive design for all screen sizes\n\nSuccess Criteria:\n- Reader understands how AI perception enables robot autonomy\n- Clear mental model of Isaac’s role in the robotics pipeline\n- Smooth transition from simulation \\(Module 02\\) to autonomy\n- Module prepares learners for VLA and LLM-based control\n- Consistent structure and tone across all modules\n\nConstraints:\n- Format: Markdown \\(.md\\), Docusaurus-compatible\n- Exactly 3 chapters\n- No hardware-specific configuration steps\n- No deep CUDA or GPU programming\n- Focus on conceptual and architectural understanding\n\nNot Building in This Module:\n- Voice co\")",
      "Bash(.specify/scripts/powershell/setup-plan.ps1 -Json)",
      "Bash(.specify/scripts/powershell/check-prerequisites.ps1 -Json)",
      "Bash(.specify/scripts/powershell/check-prerequisites.ps1 -Json -RequireTasks -IncludeTasks)",
      "Bash(npm run start)",
      "Bash(npx docusaurus start --port 3002)",
      "Bash(npm install @docusaurus/theme-mermaid)",
      "Bash(timeout 10 npx docusaurus start --port 3004)",
      "Bash(timeout 10 npx docusaurus start --port 3005)",
      "Bash(timeout 10 npx docusaurus start --port 3006)",
      "Bash(timeout 5 npx docusaurus start --port 3008)"
    ]
  }
}
