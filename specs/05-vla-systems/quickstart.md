# Quickstart Guide: Module 05 â€” Vision-Language-Action (VLA)

**Feature**: 05-vla-systems
**Created**: 2025-12-25
**Plan**: specs/05-vla-systems/plan.md

## Overview

This quickstart guide provides a high-level introduction to the Vision-Language-Action (VLA) module, designed for AI and robotics students with ROS 2 and perception knowledge. The module covers VLA systems, voice-to-action pipelines, and LLM-based cognitive planning for humanoid robots.

## Module Structure

The module consists of three chapters that build upon each other:

1. **Chapter 1**: Vision-Language-Action Systems
   - Understanding the VLA paradigm
   - Role of vision, language, and action modules
   - LLMs as planners vs controllers

2. **Chapter 2**: Voice-to-Action Pipelines
   - Speech-to-text concepts
   - Intent extraction and task decomposition
   - Voice command processing

3. **Chapter 3**: Cognitive Planning with LLMs and ROS 2
   - LLM-based planning concepts
   - Natural language to ROS 2 action sequences
   - System coordination

## Prerequisites

Before starting this module, students should have:
- Understanding of ROS 2 concepts from previous modules
- Knowledge of perception systems from the Isaac module
- Familiarity with humanoid robotics concepts

## Learning Path

### Getting Started
1. Begin with the module index page to understand learning objectives
2. Proceed through chapters sequentially to build conceptual understanding
3. Pay attention to connections between VLA components and their roles

### Key Concepts to Focus On
- The VLA paradigm for integrating vision, language, and action
- How LLMs serve as high-level planners rather than controllers
- The voice-to-action pipeline from speech to robot execution
- How LLMs coordinate perception, navigation, and manipulation

## Integration with Other Modules

This module builds on:
- **Module 01 (ROS 2)**: Expands on ROS 2 concepts with LLM integration
- **Module 04 (Isaac)**: Connects perception concepts to cognitive planning

This module prepares for:
- The autonomous humanoid capstone project
- Advanced cognitive robotics concepts

## Navigation Tips

- Use the sidebar to navigate between chapters
- Each chapter builds on the previous one
- Look for collapsible sections for advanced concepts
- Diagrams are placed close to relevant explanations

## Success Metrics

By the end of this module, you should be able to:
- Explain the Vision-Language-Action paradigm
- Understand voice-to-text pipelines using speech models
- Describe how to translate natural language goals into structured robot actions
- Explain how LLMs act as planners rather than controllers
- Conceptually integrate perception, planning, and action execution

## Technical Notes

- All content is conceptual and system-level focused
- No production-level prompt tuning included
- Focus on understanding rather than implementation
- Visual diagrams support conceptual understanding
- Technical terms are defined on first use